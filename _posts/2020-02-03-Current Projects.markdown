---
layout: post
title: <font size="6em"> <font style="color:Dodgerblue; font-family:'Gilroy Extra Bold', Gilroy;">RESEARCH & PROJECTS      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </font></font>
date: 2020-02-03
---
<br>
<font size="5em" style="color:black;">●Sensing Soft Robot (Silicon Finger with PVDF sensor)</font><font size="4em">(Mar,2020 - Nov,2020)<br></font><font size="4em" style="color:black;">Soft finger <!--automatically controlling grasping force based on it's tactile sensor   Polyvinylidene fluoride (PVDF) piezoelectric sensor embedded into the silicone finger enabled to--> sensing the bending status, touch, grip force, slip, and surface texture. This made it possible for the finger to automatically control the grip force without visual information. This can be applied to biomedical application, and industrial application.   </font><br><font size="4em" style="color:black;">-Funded by National Research Foundation of Korea & Ministry of Science and ICT<br> -<u>Won Minister of Science and ICT Award in Practical Research Competition (2 Million KRW)</u><br>-Target Conference : IROS 2021<br></font>
<iframe width="793" height="656" src="https://www.youtube.com/embed/4RQtQQPaJjY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<br>
<font size="5em" style="color:black;">●BCI As an Emotional Assistant</font><font size="4em">(Feb,2020 -)<br></font><font size="4em" style="color:black;">To make computers more empathetic and obliging to the mental wellness of users, currently developing a real-time emotion recognizing system that enables a computer to sense the user’s emotional state with around 90% accuracy, and controls the peripheral environment for their emotional recovery.</font><br><font size="4em" style="color:black;">-Target Conference : IMWUT 2022</font><br>
<img src="/images/fulls/09_1.jpg" class="image-img" width = "800"><br>
<br>
<font size="5em" style="color:black;">
●CAPTCHA PROJECT</font> <font size="4em"> (Aug,2020 - Aug,2020) <br></font><font size="4em" style="color:black;">A system that can read distorted text images</font>
<img src="/images/fulls/captcha2.jpg" class="image-img" width = "800" ><br>
<br><font size="5em" style="color:black;">
● Wireless Chest-Laminated Ultrathin and Stretchable E-Tattoo</font><font size="4em"> (Aug,2019 - Dec,2019)</font><br><font size="4em" style="color:black;"> A wireless wearable device that can perform synchronous electrocardiogram and seismocardiography measurements and extract various cardiac time intervals including systolic time interval (STI) to track heart health.<br>
</font> <font size="4em"> LU Research Group Undergraduate Research Assistant, University of Texas at Austin</font><br>
<!--<img src="/images/fulls/LU.jpg" class="image-img" width="175" height="250">-->
<hr style="height:3px">


<!-- <p style="background-color:DodgerBlue;"><font size="2em">.</font></p>-->

<br>
<br>
<h1 style="background-color:LightCyan; text-align:center; color:black;"><font size="6em">
 Brain Computer Interface As an Emotional Assistant</font></h1>
 <div style="float:right;">
<h><font size="4em">Computational Neuro Engineering(CoNE) LAB</font><br>
<br></h></div><br>
<font size="4em" style="color:black;">
<br>
<font size="4em">
Target Conference : IMWUT 2021/2022<br>
<u>Motivation & Idea:</u> These days more and more people are suffering from depression, stress and mental illness. Listening to the news about the people suffering depression due to COVID-19 quarantine, I was motivated to develop a system to help them. Also it's about time for the computers to care our mental health too.<br><br></font>

<font size="4em">
<u>Solution:</u> Drawing from the "Valence-Arousal Model", we could build a system that measures bio-signals(EEG, PPG, GSR) to recognize the “inner” emotion (as humans could control their facial expressions). If it's classified as negative valence, provide feedback (e.g., playing music or controlling lighting conditions) to help bring the user's emotion back to the desired state. <br><br>
<u>Method:</u> Through experiment, collect EEG, PPG and GSR data, then epoch it after signal processing. I extracted features from them and built a classification model with SVM. Out of "n" number of windows in a row, system counts how many windows(m) are classified as positive or negative valence. If determined as negative valence, the system controls surrounding environments. </font>
</font><br>
<br>

<img src="/images/fulls/bci1.jpg" class="image-img" width = "800"><br><font size="4em" style="color:black;">(Left) Overall classification accuracy, (Right) Accuracy per training data Length.</font><br>
<img src="/images/fulls/bci2.jpg" class="image-img" width = "800"> <br><font size="4em" style="color:black;">(Left) Overall classification accuracy, (Right) Accuracy per training data Length.</font><br>
<img src="/images/fulls/bci3.jpg" class="image-img" width = "800"> <br>


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<font size="4em" style="color:black;">
Currently accuracy of emotion classification is over 85%, and still working on improving the model. On-line experiment is planned to be held on September.
<br><br>
<br>

<u>Throughout the Project...</u> <br>
Developed Matlab function :<br>
1. <a href="/images/GSR_feature.m">Click here to download GSR feature extraction Matlab code</a><br><br>
Future project idea : &nbsp;&nbsp; "I want to combine this empathic system to our daily devices so that the users can share their “actual” emotion with others and furthermore, with things."<br>
</font>
<hr style="height:3px">










<br>
<br>
<h1 style="background-color:LightCyan; text-align:center; color:black;"><font size="5em">
 Sensing Soft Robot (Silicon Finger with PVDF sensor)</font></h1>
<div style="float:right;"><font size="4em" >
 Nanoelectronics Device LAB(NDL)</font></div>
 <br>
 <br>

 <br><font size="4em" style="color:black;">
 <u>Content & Idea:&nbsp;</u> I developed a soft robot arm with a piezoelectric sensor-based precisely controllable pneumatic finger, with the motivation to provide a physical platform for computers to interact with humans. When applied to Medical Robots or Medical catheters, minimum incision with soft movement will make interaction safer.<br></font>
 <br>
 <br>
<hr style="height:3px">
<h1 style="color:black;"><font size="5em"><u>First Model(Without PVDF Sensor), ditch outside only</u></font></h1>
<img src="/images/fingers/simul1.jpg" class="image-img" width="800"><br>
<br>
<img src="/images/fingers/simul2.jpg" class="image-img" width="800"><br>
<font size="4em" style="color:black;">↑ Simulation result shows, difference in thickness near the air channel brings different strain rate <br>             </font>
<br>
<img src="/images/fingers/1st.jpg" class="image-img" width="800"><br>

<div class="w3-content" style="max-width:400px">
  <img class="mySlides" src="/images/fingers/finger1.jpg" style="width:100%">
  <img class="mySlides" src="/images/fingers/finger2.jpg" style="width:100%">
  <img class="mySlides" src="/images/fingers/finger3.jpg" style="width:100%">
  <img class="mySlides" src="/images/fingers/finger4.jpg" style="width:100%">
</div>
<hr style="height:3px">
<script>
var slideIndex = 0;
carousel();

function carousel() {
  var i;
  var x = document.getElementsByClassName("mySlides");
  for (i = 0; i < x.length; i++) {
    x[i].style.display = "none";
  }
  slideIndex++;
  if (slideIndex > x.length) {slideIndex = 1}
  x[slideIndex-1].style.display = "block";
  setTimeout(carousel, 500);
}
</script>

<h1 style="color:black;"><font size="5em"><u> Second Model(Without PVDF Sensor) with ditch both inside and outside</u></font></h1><br>
<img src="/images/fingers/2.jpg" class="image-img" width="800"><br>
<font size="4em" style="color:black;">In order to enhance strain rate difference between upper and lower layer, also added ditch inside<br>             </font>
<img src="/images/fingers/2nd.jpg" class="image-img" width="800"><br>
<h1 style="color:black;"><font size="5em">Operation Characteristics. <br>             </font></h1>
<img src="/images/fingers/comp.jpg" class="image-img" width="800"><br>
<hr style="height:3px"><br>


<h1 style="color:black;"><font size="5em"> <u>Third Model with Embedded PVDF Sensor</u></font></h1><br>
<img src="/images/fingers/3d2.jpg" class="image-img" width="800"><br>
<font size="4em" style="color:black;">↑ 3D mold image to make a trench to embed PVDF piezoelectric sensor<br>             </font>
<img src="/images/fingers/222.jpg" class="image-img" width="800"><br>

<!-- <img src="/images/fingers/with bumps and sensor holder_2_2.jpg" class="image-img" width="300" height="200"> -->

<img src="/images/fingers/with_sensor.jpg" class="image-img" width="800" height="200"><br>
<font size="4em" style="color:black;">↑ Embedding PVDF sensor through second molding<br>             </font>
<hr style="height:3px"><br>
<img src="/images/fingers/setting.jpg" class="image-img" width="900"><br>
<br>
<br><br>
<img src="/images/fingers/output2.jpg" class="image-img" width="900"><br>
<img src="/images/fingers/output3.jpg" class="image-img" width="500"><br>
<font size="4em" style="color:black;">↑ Through embedded sensor output, 1. bending state and 2.touching can be monitored<br>             </font>
<br>

<iframe width="898" height="656" src="https://www.youtube.com/embed/ETY7hvVFddA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<br>
<img src="/images/fingers/slip.jpg" class="image-img" width="900">
<font size="4em" style="color:black;">↑ Through sensor output, slip can be detected <br> </font>

<hr style="height:3px"><br>
<h1 style="color:black;"><font size="5em"><u> Fourth Model Fully Automated by PVDF Sensor</u></font></h1><br>
<iframe width="793" height="656" src="https://www.youtube.com/embed/4RQtQQPaJjY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<font size="4em" style="color:black;">
<br>
By monitoring the sensor output, and together with microcontroller, finger controls the grasping force automatically by stop inflating when it has grasped the item with enough force, based on its tactile senses. Also, if the gripping object slips, it applies additional force.</font>
<hr style="height:3px">

<h1 style="color:black;"><font size="5em"> Moreover, with the same sensor, surface texture sensing is also possible. </font></h1><br>
<img src="/images/fingers/texture1.jpg" class="image-img" width="900"><br>
<img src="/images/fingers/texture2.jpg" class="image-img" width="900">
<font size="4em" style="color:black;">↑ By attaching biomimetic fingerprint, surface texture detecting is possible.<br> </font>

<hr style="height:3px"><br>
<h1 style="color:black;"><font size="5em"><u> Connecting the soft finger to an soft arm</u></font></h1><br>
<img src="/images/fingers/new/arm5.jpg" class="image-img" width="900"><br>
<font size="4em" style="color:black;">↑ Through combining this soft finger to a soft robot arm broadened the sensing boundary<br>            
<br>
<br>
</font>
<br><br>
<br>
<br>
<br>
<hr style="height:3px;">










<br>
<h1 style="background-color:LightCyan; text-align:center; color:black;"><font size="5em">
CAPTCHA PROJECT - Personal Project
</font></h1>
<div style="float:right;"><font size="4em"> AUG,2020 </font></div>
<br>
<font style="color:black;">
<font size="5em">Developed a system that can read distorted text with Machine Learning</font><br>
Used the same images that "Keras (OCR model for reading Captchas))" used<br>
<font size="4em">
<u>Motivation:</u> Understanding Machine Learning theoretically and implementing the idea through python to acquire practical machine learning techniques.<br><br>
<u>Promblem:</u> Small amount of data sets that caused over fitting and low accuracy.<br><br>
<u>Idea & Solution:</u> In order to use this model in every other CAPTCHA images, I thought of slicing the image into single characters and train each of them into 36(number of alphabets + 0~9 numbers) classes. Also, through data augmentation, I multiplied training data to prevent over fitting and improve accuracy to <font size="4em"><u>93%</u></font>. <br>
</font></font>
<br>
<br>
<img src="/images/fulls/captcha1_1.jpg" class="image-img" width="800"><br>
<img src="/images/fulls/captcha1_2.jpg" class="image-img" width="800"><br>
<hr style="height:3px"><br>
<img src="/images/fulls/captcha2.jpg" class="image-img" width="800"><br>
<hr style="height:3px"><br>
<h1 style="color:black;"><font size="5em">Result</font></h1>
<img src="/images/fulls/captcha3.jpg" class="image-img" width="800" height="600"><br>
<hr style="height:3px">
<br>








<!--
<br>
<h1 style="background-color:LightCyan; text-align:center; color:black;"><font size="6em">
VLSI (EE 460R)Course Project </font></h1>
<div style="float:right;"><font size="5em"> University of Texas at Austin (Aug,2019 – Dec,2019)
</font></div>
<br><br>

<font size="5em" style="color:black;">
<u>"SSP(Synchronous Serial Port) Design” with Verilog</u><br></font>
<br>
<img src="/images/fulls/VLSI4.jpg" class="image-img" width="500" height="300"><br>
Sketch ↑ <br>
<img src="/images/fulls/Transmiter_part.jpg" class="image-img" width="800" height="100"><br>
Transmit Works ↑<br>
<img src="/images/fulls/FIFO.jpg" class="image-img" width="800" height="300"><br>
FIFO Works ↑<br>
<img src="/images/fulls/Transmiter_Reciever_connection.jpg" class="image-img" width="800" height="300"><br>
Receiver Works and Connected to Transmitter ↑<br>
<img src="/images/fulls/SSP.jpg" class="image-img" width="800" height="800"><br>
SSP Works (Data out at the top, Data in at the bottom) ↑<br>
<a href="/images/Hoonjin_Jung_lab3A/RxFIFO.v">Click here for the RxFIFO Verilog code</a><br>
<a href="/images/Hoonjin_Jung_lab3A/RxLogic.v">Click here for the RxLogic Verilog code</a><br>
<a href="/images/Hoonjin_Jung_lab3A/SSP.v">Click here for the SSP Verilog code</a><br>
<a href="/images/Hoonjin_Jung_lab3A/TxFIFO.v">Click here for the TxFIFO Verilog code</a><br>
<a href="/images/Hoonjin_Jung_lab3A/TxLogic.v">Click here for the TxLogic Verilog code</a><br>
<br><br>
<font size="5em" style="color:black;">
<u>"SSP with ARM using WISHBONE Design” with Verilog</u><br></font><br>
<img src="/images/fulls/ARM.jpg" class="image-img" width="500" height="300">
<img src="/images/fulls/ARM2.jpg" class="image-img" width="500" height="300"><br>
Sketch ↑ <br>
<a href="/images/Hoonjin_Jung_lab3B/WISHB_MASTER.v">Click here for the WISHB_MASTER Verilog code</a><br>
<a href="/images/Hoonjin_Jung_lab3B/WISHB_SLAVE.v">Click here for the WISHB_SLAVE Verilog code</a><br>
<br>
<br>
<font size="5em" style="color:black;"><u>"16-bit Adder Design” with Virtuoso</u><br>
Kogge-Stone Adder</font><br>
<img src="/images/fulls/Kogge.jpg" class="image-img" width="500" height="300"><br>
<a href="/images/jung_hoonjin_2a.tar.gz.zip">Click here to Download the .tar(Virtuoso) file</a><br>
<br>
<br>
<font size="5em" style="color:black;">
<u>"1-bit Memory Design” with Virtuoso</u><br></font>
<br>
<img src="/images/fulls/VLSI1.jpg" class="image-img" width="500" height="300">
<br>
Schematic ↑ <br>


<hr style="height:3px">
<br>
-->





<br>
<h1 style="background-color:LightCyan; text-align:center; color:black;"><font size="5em">
LU Research Group Undergraduate Research Assistant </font></h1>
<div style="float:right;"><font size="4em"> University of Texas at Austin (Aug,2019 – Dec,2019)
</font></div>
<br>

<font size="4em" style="color:black;">
&nbsp;•	Participated in developing and modifying the design of the circuits of wireless chest laminated &nbsp;ultrathin, stretchable E-Tatto that measures and combines ECG and SCG(Seismocardiogram) &nbsp;capabilities to track heart health.<br>
</font>
<br>
<div style="float:left;">
<img src="/images/fulls/LU.jpg" class="image-img" width="250" height="350">
</div>
<font size="4em"><b><a href="https://lu.ae.utexas.edu/index.php/98-feature/219-a-chest-laminated-ultrathin-and-stretchable-e-tattoo" target="_blank"><u>
&nbsp;"A Chest-Laminated Ultrathin and Stretchable E-Tattoo for the Measurement of Electrocardiogram, Seismocardiogram, and Cardiac Time Intervals”<br></u></a></b></font>
<font size="3em">
&nbsp;T. Ha, J. Tran, S. Liu, H. Jang, H. Jeong, R. Mitbander, H. Huh, Y. Qiu, J. Duang, R. Wang, P. Wang, A. Tandon, J. Sirohi, N. S. Lu*,  Advanced Science, 1900290, May 2019 <br>
</font>
<br>
<br>
<br><br><br><font style="color:black;"><br><br>
&nbsp; • Design the circuit for the bluetooth layer &nbsp;stretchable.<br></font>
<img src="/images/fulls/lu11.jpg" class="image-img" width="250" height="300">
<img src="/images/fulls/lu22.jpg" class="image-img" width="400" height="300">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<font style="color:black;">
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;former version(left)  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp; latter version(right)<br>
<br>
• Soldering<br></font>
<img src="/images/fulls/LU7.jpg" class="image-img" width="350" height="250">
<br>
<hr style="height:3px">
<br><br>
